Abstract—Skin lesion boundary segmentation remains a central problem in melanoma analysis, yet many reported benchmarks suffer from inconsistent experimental splits, limited reproducibility, and high computational demands. In this work, we present a comprehensive evaluation of segmentation architectures on the ISIC-2018 lesion segmentation challenge, establishing deterministic data splits and fixed seeds to ensure exact repeatability. Our evaluation shows that DuaSkinSeg achieves the highest performance (87.85% Dice), while our Lightweight DuaSkinSeg variant maintains competitive accuracy (87.72% Dice) with 73% fewer parameters (8.4M vs 31.2M). For more constrained environments, Custom U-Net (86.30% Dice, 4.3M parameters) and MONAI U-Net (84.50% Dice, 2.6M parameters) remain viable options. Our fine-tuned UNetMamba model shows meaningful improvements (81.61% Dice, +0.61% over baseline) through progressive unfreezing, as documented in our training logs, though it still lags behind transformer-based approaches. Our Enhanced Ensemble (87.53% Dice) demonstrates particular strength in boundary detection (0.1502 Boundary IoU), while maintaining deployment feasibility on consumer-grade hardware (≤8GB GPU). We provide detailed computational tradeoffs in parameters, FLOPs, GPU memory usage and inference time, alongside analysis of common failure modes including small lesions and irregular boundaries. To facilitate reproducibility, we release our code, data splits, and trained checkpoints, offering practical guidance for researchers with limited computational resources.
Keywords—Skin lesion segmentation, ISIC-2018 challenge, U-Net variants, Medical image segmentation, Reproducible benchmarks, Resource-constrained deep learning
I.	INTRODUCTION
Skin cancer, particularly melanoma, is a leading cause of morbidity and mortality worldwide, and early detection through automated lesion analysis is crucial. Dermoscopic image segmentation of skin lesions enables precise measurement and feature extraction, aiding diagnosis. The ISIC dermoscopy dataset series has become a cornerstone for benchmarking these algorithms, providing extensive annotated images that have driven significant advances in dermatological imaging AI. In this context, U-Net and its variants are ubiquitous due to their ability to capture fine-grained details. The classic U-Net architecture uses a contracting path to capture context and a symmetric expanding path for accurate localization. Many works build upon this core, for example, adding attention gates or leveraging frameworks like MONAI for ease of development.
However, recent literature reveals two persistent issues. First, reproducibility is often limited: studies may omit precise train/val/test splits or random seeds, making exact replication impossible. Second, state-of-the-art models often demand substantial compute resources. For instance, hybrid architectures using Mamba state-space blocks (e.g., DermoMamba) report high Dice scores (~0.88-0.90 on ISIC data) but typically require significant GPU memory. Meanwhile, Transformer-based models like DuaSkinSeg achieve excellent performance (87.85% Dice) but at the cost of large parameter counts (31.2M). These trends can exclude researchers with limited hardware.
In this work, we address these issues by providing a fully deterministic, open benchmark of lightweight segmentation models and their ensembles on the ISIC-2018 Task 1 challenge. Our comprehensive evaluation shows that while the standard DuaSkinSeg model achieves the highest performance (87.85% Dice), our Lightweight DuaSkinSeg variant offers a compelling alternative with minimal performance reduction (87.72% Dice) despite using 73% fewer parameters (8.4M vs 31.2M) and 38% less GPU memory. For even more constrained environments, our Custom U-Net (86.30% Dice, 4.3M parameters) and MONAI U-Net (84.50% Dice, 2.6M parameters) remain viable options. Fine-tuned UNetMamba models demonstrate meaningful improvements (81.61% Dice, +0.61% over baseline) but still lag behind transformer-based approaches.
Our contributions are: (1) Reproducible Baselines: We fix data splits and random seeds to enable exact repeatability; (2) Comprehensive Benchmarking: We evaluate multiple architectures including U-Net variants, Transformer-based models, and State Space Models with detailed reporting of segmentation accuracy and resource requirements; (3) Lightweight Model Optimization: We demonstrate that parameter-efficient models can achieve near state-of-the-art performance while maintaining low memory footprints; (4) Resource Metrics: We quantify trade-offs by reporting model size, FLOPs, GPU memory usage, and inference speed; (5) Failure Analysis: We analyze segmentation performance across challenging cases including small lesions, irregular boundaries, and hair occlusions; and (6) Open Source: We release all code, data splits, and trained checkpoints for exact reproducibility.
The rest of the paper is organized as follows. Section 2 reviews related work on segmentation architectures and benchmarks. Section 3 describes our data, preprocessing, architectures, loss functions, optimization strategy, and experimental setup. Section 4 presents experimental results. Section 5 discusses key insights, practical implications, and limitations. Finally, Section 6 concludes.
II.	RELATED WORK
A.	Skin Lesion Segmentation Approaches
Early approaches to skin lesion segmentation relied on traditional image processing techniques including thresholding [X], region growing [Y], and active contours [Z]. While these methods work well for high-contrast lesions, they struggle with heterogeneous pigmentation and irregular borders common in melanomas. The introduction of deep learning approaches, particularly CNN-based segmentation networks, has substantially improved performance on challenging cases.
B.	U-Net Variants in Medical Image Segmentation
The original U-Net architecture [Ronneberger et al., 2015] consists of a contracting path to capture context and a symmetric expanding path for precise localization. MONAI U-Net [X] adapts this architecture with domain-specific optimizations for medical imaging, including residual connections and advanced normalization techniques. Attention U-Net [Y] incorporates attention gates that highlight salient features and suppress irrelevant regions, showing particular benefits for boundary detection in heterogeneous lesions.
C.	State Space Models and Transformer Approaches
Recently, transformer-based architectures have shown promising results in medical image segmentation. Models like TransUNet [X] and SwinUNet [Y] leverage self-attention mechanisms to capture long-range dependencies. DuaSkinSeg [Z] combines a lightweight Vision Transformer with a CNN encoder for efficient feature extraction. Simultaneously, State Space Models (SSMs) have emerged as alternatives to transformers, with architectures like DermoMamba [A] and UNetMamba [B] reporting competitive performance with potentially reduced computational requirements.
D.	Ensemble Approaches in Medical Image Segmentation
Ensemble methods have consistently demonstrated improved performance in medical image segmentation [X, Y]. These approaches typically combine predictions from multiple models through averaging [Z], weighted fusion [A], or meta-learning approaches [B]. While ensembles generally improve robustness to variations in lesion appearance, they traditionally come with increased computational costs during both training and inference, limiting their practical deployment.
E.	Reproducibility Challenges in Medical Image Segmentation
Despite advances in skin lesion segmentation, reproducibility remains a significant challenge. Many studies report results without specifying exact data splits [X], random seeds [Y], or complete implementation details [Z]. This hampers direct comparison and impedes scientific progress. Recent initiatives like MedPerf [A] and MONAI Label [B] have attempted to standardize evaluation, but widespread adoption remains limited. Additionally, the trend toward increasingly complex models with substantial computational requirements (e.g., [C] requiring 16GB+ GPU memory) creates barriers to reproducibility for researchers with limited resources.
F.	Efficient Deep Learning for Medical Imaging
The demand for efficient deep learning models has led to several parameter-efficient architectures for medical image segmentation. EfficientUNet [X] and MobileNetV2-UNet [Y] demonstrate competitive performance with reduced parameter counts. Orthogonally, model compression techniques including pruning [Z], quantization [A], and knowledge distillation [B] offer pathways to reduce computational requirements of existing architectures. Beyond accuracy, recent work has begun to systematically report efficiency metrics including FLOPS, memory usage, and inference time [C, D], highlighting the importance of these considerations for real-world deployment.
III.	METHODOLOGY
A.	Dataset and Preprocessing
Our study utilizes the ISIC 2018 Challenge Task 1 dataset, which contains 2,594 dermoscopic images with corresponding ground truth segmentation masks. We implemented a robust preprocessing pipeline to ensure consistent input for all models, addressing common challenges in dermoscopic imaging. 
Data Preprocessing Protocol: All images undergo a standardized pipeline including resolution normalization to 384×384 pixels with aspect ratio preservation, artifact removal (rulers, color calibration charts), color normalization, and optional hair removal through inpainting techniques. During validation, we identified and corrected issues with 140 problematic images that had corrupted masks or significant artifacts, ensuring consistent quality across the dataset.
Reproducible Data Splits: To ensure exact reproducibility, we created deterministic data splits using fixed random seeds (seed=42), resulting in:
•	Training set: 2,293 images (80%)
•	Validation set: 301 images (10%)
•	Test set: 1,000 images (ISIC 2018 challenge test set)
These splits are preserved and shared in our code repository to enable precise replication of our results. We implemented a consistent augmentation pipeline including random flips, rotations (±30°), brightness/contrast adjustments, and elastic deformations, applied only during training with fixed seeds to ensure reproducibility.

B.	Model Architectures
We implemented and evaluated several model architectures with varying complexity and parameter efficiency.
1)	U-Net Variants
U-Net: Our baseline follows the original U-Net architecture with modern optimizations including batch normalization and residual connections. It uses a five-level encoder-decoder structure with skip connections and features [32, 64, 128, 256, 512]. This model contains 4.3M parameters and serves as our foundational architecture.
MONAI U-Net: Leveraging the medical imaging-optimized framework, this variant incorporates domain-specific enhancements including residual units, advanced normalization, and optimized memory usage. With only 2.6M parameters, it represents our most lightweight option.
Attention U-Net: This enhanced architecture incorporates attention mechanisms to highlight salient features and suppress irrelevant regions. It integrates CBAM (Convolutional Block Attention Module) in the skip connections to improve boundary detection. With 57.8M parameters, it represents our largest U-Net variant.
2)	Advanced Architectures
DuaSkinSeg: This dual-encoder architecture combines a MobileNetV2 CNN path with a Vision Transformer encoder path, followed by feature fusion and a specialized decoder with skip connections. The standard version contains 31.2M parameters.
Lightweight DuaSkinSeg: We developed a parameter-efficient variant of DuaSkinSeg that reduces the model size by 73% (to 8.4M parameters) through architectural optimizations including reduced transformer embedding dimensions (from 384 to 192), fewer attention heads (from 12 to 6), and lighter fusion modules.
UNetMamba: This architecture integrates Mamba State Space Model blocks into the U-Net structure, replacing traditional convolutional blocks in the bottleneck and decoder pathways with 2D-adapted Mamba units that process image data along both height and width dimensions sequentially.
3)	Ensemble Method
We implemented a lightweight ensemble approach that combines predictions from multiple models:
Enhanced Ensemble: Our final ensemble combines two Attention U-Net variants and two Custom U-Net variants, using weighted averaging based on validation performance. We incorporate test-time augmentation (TTA) with flips and rotations to further improve robustness.
C.	Training and Optimization Strategy
1)	Loss Functions
We evaluated multiple loss functions to optimize boundary segmentation:
•	Dice Loss: Optimizes overlap between predicted and ground truth masks
•	Combined Loss: Weighted combination of BCE and Dice (weights: 0.5, 0.5)
•	Boundary-Aware Loss: Enhanced version with specific boundary term (weights: 0.4 BCE, 0.4 Dice, 0.2 Boundary)
•	Advanced Combined Loss: Multi-term loss with specialized boundary enhancement
D.	Lightweight Ensembles
To mitigate the weaknesses of individual models, we construct simple ensembles of U-Net variants. Ensembles are known to improve robustness and accuracy in segmentation. We experiment with ensembling the outputs of multiple independently trained models (e.g. two Attention U-Nets and one baseline U-Net). Two ensemble strategies are considered: (a) Logit averaging: we average the raw logits (pre-sigmoid) from each model and then threshold; (b) Majority voting: we convert each model’s sigmoid output to a binary mask and take the pixel-wise majority vote. Both methods are lightweight: they require no additional training and incur only minor extra inference time. By combining models that may err on different images, the ensemble often achieves higher overall Dice than any single model. This approach has been shown to yield performance gains in U-Net cascades and parallel U-Net systems.
E.	Optimization & Reproducible training
All models were trained with Adam (initial lr = 1e-3) and a cosine-annealing scheduler; early stopping (patience = 10 epochs) was used to avoid overfitting. Fine-tuning used a progressive unfreeze: Phase 1: freeze the encoder and train the decoder for 10 epochs; Phase 2: unfreeze the encoder using a lower encoder lr (0.1×) with layer-wise learning rates for encoder/decoder. This protocol yielded a +0.61% Dice improvement for the UNetMamba run. For reproducibility we fixed random seeds (torch, numpy, random), set cudnn.deterministic = True, and kept exhaustive logging, checkpointing, and version-controlled configs.
F.	Evaluation Framework
Segmentation metrics: We report standard overlap and boundary metrics: Dice (primary), IoU (Jaccard), Pixel Accuracy, Sensitivity (Recall), Specificity, Precision, Boundary IoU, and Hausdorff-95. Dice and IoU quantify overall overlap; Boundary IoU and HD-95 capture boundary quality and worst-case deviations.
Computational metrics: To characterize efficiency and deployability we measure: trainable parameters, model size (MB), peak GPU memory (GB) during inference, per-image inference time (s), and FLOPs. These allow direct comparison of accuracy vs resource cost and construction of Pareto frontiers.
G.	Experimental Setup
All experiments ran on a single NVIDIA GTX 1070 (8 GB VRAM) using PyTorch 2.1.0 with CUDA 13.0; mixed precision (AMP) was enabled to reduce memory use. Models were trained for up to 100 epochs with early stopping (patience = 10) and batch sizes tuned per model (4–12 to fit memory). Training logs, validation curves, and checkpoints were recorded regularly (the UNetMamba fine-tuning run, for example, progressed 33 epochs and stopped by early stopping). To enable exact replication we release all code, data splits, configs, and pretrained checkpoints.
